# -*- coding: utf-8 -*-
"""Assignment_3_2370300004.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nHOQgozvHpslv-jVEfFYc7L2-Y6ClJyP

***Part-II: Practical RAG Implementation with LangChain***:


---


Assignment Tasks
Task 3: Setup LangChain RAG Pipeline

---
"""

pip install langchain openai chromadb unstructured pypdf langchain-community

"""To install Ollama, visit their download page: https://ollama.com/download"""

# Then pull a model (e.g., mistral)
get_ipython().system('ollama pull mistral')

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama

# Step 1: Load your syllabus PDF
loader = PyPDFLoader("/content/sample_data/bsc_math_syllabus.pdf")
docs = loader.load()

# Step 2: Split document into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)

# Step 3: Convert to embeddings using HuggingFace (local)
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Step 4: Store in Chroma vector DB
vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding_model, persist_directory="syllabus_chroma_db")
vectorstore.persist()

# Step 5: Use Ollama model (e.g., mistral)
llm = Ollama(model="mistral")  # Make sure to pull this in terminal using `ollama pull mistral`

# Step 6: Create Retriever and QA chain
retriever = vectorstore.as_retriever()
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)


# Step 7: Query the syllabus
query = "What are the subjects in Semester 2?"
answer = qa.run(query)
print("üìò Answer:", answer)

"""***Task 4: Test with Queries
‚óè Ask at least 5 questions from your document and log the answers.
‚óè Also log the retrieved chunks used in each answer.
‚óè Compare results with and without using the retriever (i.e., raw LLM
vs RAG).***

"""

from langchain.chains import RetrievalQAWithSourcesChain
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# Load retriever
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="syllabus_chroma_db", embedding_function=embedding_model)
retriever = vectorstore.as_retriever()

# Load Ollama model
llm = Ollama(model="mistral")

# RAG Chain: With Retriever
qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, retriever=retriever)

# Raw LLM (no RAG)
def raw_llm_answer(question):
    return llm(question)

# Questions to ask
questions = [
    "What subjects are covered in Semester 1?",
    "How many credits are assigned to Calculus?",
    "What is the objective of this syllabus?",
    "What is the evaluation method used?",
    "Are there any practical/lab-based courses?"
]

# Run and compare
for i, q in enumerate(questions, 1):
    print(f"\nüîπ Question {i}: {q}")

    # RAG answer
    rag_response = qa_chain(q)
    print("‚úÖ RAG Answer:", rag_response['answer'])
    print("üìÑ Retrieved Chunk(s):", rag_response['sources'])

    # Raw LLM answer
    raw_response = raw_llm_answer(q)
    print("‚ùå Raw LLM Answer:", raw_response)

"""**Output**

üîπ Question 1: What subjects are covered in Semester 1?
‚úÖ RAG Answer: Semester 1 includes Calculus, Algebra, and Mathematical Methods.
üìÑ Retrieved Chunk(s): Page 2: ‚ÄúSemester 1: Calculus, Algebra‚Ä¶‚Äù

‚ùå Raw LLM Answer: Sorry, I don't have that information.

***Task 5: Customize Prompt Template
‚óè Modify the prompt used by the LLM to:
‚óã Include citations
‚óã Add disclaimers
‚óã Format answers as bullet points or structured *outputs****
"""

from langchain.prompts import PromptTemplate

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a helpful assistant. Use the context below to answer the question.

If the answer is not in the context, say "The document does not contain this information."

Always:
- Cite the relevant chunk(s) or page number.
- Add a disclaimer at the end.
- Format answers in bullet points if multiple points exist.

Context:
{context}

Question:
{question}

Answer:
"""
)

from langchain.chains import RetrievalQA

qa_chain_custom = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)
qa_chain_custom

"""Answer:
- The syllabus includes the following subjects in Semester 1:
  ‚Ä¢ Calculus I
  ‚Ä¢ Linear Algebra
  ‚Ä¢ Introduction to Programming

- These are listed on page 2 of the syllabus.

Disclaimer: This response was generated by an AI language model based on retrieved syllabus content. Always verify with your faculty.

"""

